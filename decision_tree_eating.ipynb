{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9dc25-1b38-4db6-8377-e1607ac9c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avcv.all import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eac48b-faf1-4050-bf6c-146ba92eea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label:\n",
    "    def __init__(self, label, video):\n",
    "        self.label = label\n",
    "        self.video = video\n",
    "        \n",
    "    def check_action_at_frame_idx(self, i):\n",
    "        if isinstance(i, int):\n",
    "            i = i/self.video.fps\n",
    "            \n",
    "        actions = []\n",
    "        for action in self.label['annotation']['actionAnnotationList']:\n",
    "            if i >= action['start'] and i < action['end']:\n",
    "                action_idx = action['action']\n",
    "                action_name = self.actionid2name[action_idx]\n",
    "                actions.append(action_name)\n",
    "        return actions\n",
    "    @property\n",
    "    def actionid2name(self):\n",
    "        if hasattr(self, '_actionid2name'):\n",
    "            return self._actionid2name\n",
    "        ret = dict()\n",
    "        for actionLabel in self.label['config']['actionLabelData']:\n",
    "            ret[actionLabel['id']] = actionLabel['name']\n",
    "        self._actionid2name = ret\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b66e82-7a4c-4c6f-b507-b493d4f87155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(label_path, label_type='food'):\n",
    "    if label_type == 'food':\n",
    "        video_name = get_name(label_path)\n",
    "        pred_json_path = f'/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/{video_name}/annotations/pred_mb2_face_food.json'\n",
    "        root_video_name = '_'.join(video_name.split('_')[:-2])\n",
    "        video_path = label_path.replace('.json', '.mp4')\n",
    "        raw_feat_path = pred_json_path.replace('.json', '_2_raw_outputs.pkl')\n",
    "    elif label_type == 'phone/cigarret':\n",
    "        pred_json_path = f'/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/{video_name}/annotations/pred_mb2_face_food.json'\n",
    "        t = 'smoking' if 'smoking' in video_name else 'mobile_usage'\n",
    "        label_path = f'/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/training/yoon/{t}/{video_name}.json'\n",
    "        video_path = label_path.replace('.json', '.mp4')\n",
    "        raw_feat_path = pred_json_path.replace('.json', '_2_raw_outputs.pkl')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    assert osp.exists(pred_json_path), f'404 {pred_json_path}'\n",
    "    assert osp.exists(label_path), f'404 {label_path}'\n",
    "    assert osp.exists(video_path), f'404 {video_path}'\n",
    "    assert osp.exists(raw_feat_path), f'404 {raw_feat_path}'\n",
    "    \n",
    "    return dict(\n",
    "        pred_json_path=pred_json_path,\n",
    "        label_path = label_path,\n",
    "        video_path=video_path,\n",
    "        raw_feat_path=raw_feat_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59871c92-bf9f-4db0-a692-70a37d09d3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 /home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0017/annotations/pred_mb2_face_food_2_raw_outputs.pkl\n",
      "404 /home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hoangnh42_Sensing_Session0_CAMc_1b_2b_3a_4c_5b_6b_7b_8a_9a_10b_11b_12b_13b_14a_15b_16f_17a_18a_19a_20b_eating_0011/annotations/pred_mb2_face_food_2_raw_outputs.pkl\n",
      "len(label_paths)=67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_json_paths = []\n",
    "\n",
    "\n",
    "label_paths  = glob('/data/DMS_Behavior_Detection/RawVideos/Action_Eating/**/*.json', recursive=True)\n",
    "for label_path in label_paths:\n",
    "    try:\n",
    "        json_paths = get_data(label_path, 'food')\n",
    "        list_json_paths.append(json_paths)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "label_paths  = glob('/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/*/**/*.json', recursive=True)\n",
    "print(f'{len(label_paths)=}')\n",
    "for label_path in label_paths:\n",
    "    try:\n",
    "        json_paths = get_data(label_path, 'food')\n",
    "        list_json_paths.append(json_paths)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "len(list_json_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6fdc1-b7d3-4a4d-a43b-a37012352265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_json_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/annotations/pred_mb2_face_food.json',\n",
       " 'label_path': '/data/DMS_Behavior_Detection/RawVideos/Action_Eating/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003.json',\n",
       " 'video_path': '/data/DMS_Behavior_Detection/RawVideos/Action_Eating/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003.mp4',\n",
       " 'raw_feat_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/annotations/pred_mb2_face_food_2_raw_outputs.pkl'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_json_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136e858-8416-45a1-9633-505ad350fc86",
   "metadata": {},
   "source": [
    "## Read 2d video feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eeffe-cb3a-4757-b855-eae0aeb39253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_flatten_to_2d_feature(flatten_sample):\n",
    "    feat_sizes = [416//8, 416//16, 416//32]\n",
    "    # reg_orig_shape = np.array(reg_orig_shape)**2\n",
    "    cur_i = 0\n",
    "    feats = []\n",
    "    for feat_size in feat_sizes:\n",
    "        a = cur_i\n",
    "        b = a+feat_size**2\n",
    "        cur_i = b\n",
    "        feats.append(flatten_sample[a:b].reshape(feat_size, feat_size, -1))\n",
    "    return feats\n",
    "\n",
    "def read_raw_feat_one_video(path):\n",
    "    data = dict(mmcv.load(path))\n",
    "    # for k in data:\n",
    "    #     data[k] = convert_flatten_to_2d_feature(data[k])\n",
    "    return data\n",
    "    # list_json_paths[0]['raw_feat_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299aeee-1815-4238-b023-9400d8dbab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ddfbe-8cea-4686-8dd3-e6504d19d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anns2tensor(img, anns):\n",
    "    h, w = img['height'], img['width']\n",
    "    tensor = []\n",
    "    for ann in anns:\n",
    "        x,y,w,h = ann['bbox']\n",
    "        x /=img['width']\n",
    "        w /=img['width']\n",
    "        y /=img['height']\n",
    "        h /=img['height']\n",
    "        s = ann['score']\n",
    "        cat = ann['category_id']\n",
    "        tensor.append([x,y,w,h,s, cat])\n",
    "    return np.array(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05454863-cca5-436e-8c2e-3b5152f0ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [01:41<00:00,  3.84it/s]2022-09-20 05:01:48.873 | INFO     | avcv.process:multi_thread:33 - multi_thread\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [01:41<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def collect_data(inp):\n",
    "    index, json_paths = inp\n",
    "    v = mmcv.VideoReader(json_paths['video_path'])\n",
    "    # t = len(v)\n",
    "    label = Label(mmcv.load(json_paths['label_path']), v)\n",
    "    cc = CocoDataset(json_paths['pred_json_path'])\n",
    "    data = []\n",
    "    raw_feat2d = read_raw_feat_one_video(json_paths['raw_feat_path'])\n",
    "    for i, frame in enumerate(v):\n",
    "        actions = label.check_action_at_frame_idx(i)\n",
    "        anns = cc.gt.imgToAnns[i]\n",
    "        img = cc.gt.imgs[i]\n",
    "        tensor = None#anns2tensor(img, anns)\n",
    "        img_path = osp.join(cc.img_dir, img['file_name'])\n",
    "        \n",
    "        data.append((tensor, actions[0] if len(actions) else 'none' , \n",
    "                     index, img_path, img['id'], raw_feat2d[img['id']]))\n",
    "        \n",
    "    return data\n",
    "data = multi_thread(collect_data, list(enumerate(list_json_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0160a-4ccd-4919-9d94-7de6c3721474",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for _ in data: all_data += _\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=['tensor', 'action', 'video_index', 'img_path', 'img_id', 'feat1d'])\n",
    "ids = df[df['action'] == 'smocking'].index\n",
    "df.loc[ids, 'action'] = 'smoking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb65c5f-d901-4638-8687-6f77e99a4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = get_exp_by_file('exps/dms/mb2_face_food.py')\n",
    "# model = exp.get_model()\n",
    "# st = torch.load('./YOLOX_outputs/mb2_face_food/best_ckpt.pth')['model']\n",
    "# res = model.load_state_dict(st)\n",
    "# print(res)\n",
    "# model.requires_grad_(False).eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e797d91-ff30-46a1-beb8-d24b1626c860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eating': 0, 'none': 1, 'mobile usage': 2, 'smoking': 3}\n"
     ]
    }
   ],
   "source": [
    "_id2action = dict(enumerate(df['action'].apply(str).unique().tolist()))\n",
    "action2id = {v:k for k, v in _id2action.items()}\n",
    "print(action2id)\n",
    "\n",
    "def get_y(actions):\n",
    "    return action2id[str(actions)]\n",
    "\n",
    "def get_x_tensor(tensor):\n",
    "    zt = np.zeros([1,6], dtype=np.float32)\n",
    "    def get_tensor_cat(cat):\n",
    "        if len(tensor) == 0:\n",
    "            return zt\n",
    "        \n",
    "        _t = tensor[tensor[:,-1] ==cat]\n",
    "        \n",
    "        if len(_t):\n",
    "            max_id = _t[:,-2].argmax()\n",
    "            _t = _t[max_id][None]\n",
    "        else:\n",
    "            return zt\n",
    "            \n",
    "        return _t\n",
    "    return np.concatenate([get_tensor_cat(i) for i in range(1, 7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93e5ba-2878-4c73-95e9-64f079f6d91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b296985-b203-46ef-897d-65dcf49c1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x'] = df.feat1d#multi_process(get_x, df.tensor, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6a2c8-9302-4dce-a91c-429bafafb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = df['action'].apply(get_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a047d-8962-45d5-84ab-caf90b40561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0888634-9639-4a57-bfd2-6753f03ccbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eating', 'none', 'mobile usage', 'smoking']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.action.unique().tolist()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8c818-e077-407d-a4d4-9a5536966ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df_train)=65748, len(df_val)=18277\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df.video_index.apply(lambda i: i % 5 != 0)]\n",
    "df_val = df[df.video_index.apply(lambda i: i % 5 == 0)]\n",
    "print(f'{len(df_train)=}, {len(df_val)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffad62c-9d08-4c43-935e-3af92efc5a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c89bdc-756e-4823-a0d2-81f96dbbe23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d36a1-9ba0-48d7-8116-cff26536a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2xy(df):\n",
    "    return np.array(df.x.values.tolist()).reshape(len(df), -1), np.array(df.y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2387ec8-300d-4ad4-a8c3-3c46ef32fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = df2xy(df_train)\n",
    "xval, yval = df2xy(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882436d5-e6bf-484c-840b-7ef8a2cb096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937d98d-0461-44c8-9c57-d26c7376e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_diabetes\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# # regressor = DecisionTreeRegressor(random_state=0)\n",
    "# # cross_val_score(regressor, X_train, y_train, cv=10)\n",
    "\n",
    "# clf = DecisionTreeClassifier(max_depth=3, random_state = 42)\n",
    "\n",
    "# clf.fit(xtrain, ytrain)\n",
    "\n",
    "# list_new_categories = ['cigarette', 'food/drink', 'phone', 'face', 'eye', 'mouth']\n",
    "\n",
    "# len(df_train), len(df_val)\n",
    "\n",
    "# list_cats = [_['name'] for _ in mmcv.load(list_json_paths[0]['pred_json_path'])['categories']]\n",
    "\n",
    "# from sklearn import tree\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(30,10), facecolor ='k')\n",
    "# feature_names = []\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         part = list_cats\n",
    "#         name = ['x', 'y', 'w', 'h', 'score', 'category'][j]\n",
    "#         feature_names.append('{}-{}'.format(part, name))\n",
    "\n",
    "# a = tree.plot_tree(clf,\n",
    "\n",
    "#                    feature_names = feature_names,\n",
    "\n",
    "#                    class_names = labels,\n",
    "\n",
    "#                    rounded = True,\n",
    "\n",
    "#                    filled = False,\n",
    "\n",
    "#                    fontsize=14\n",
    "#                   )\n",
    "# plt.show()\n",
    "# test_pred_decision_tree = clf.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38094cc1-180a-482a-89e9-93f2bc3b11a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action\n",
       "eating           5358\n",
       "mobile usage    22372\n",
       "none            50551\n",
       "smoking          5744\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('action').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab595b0e-9f78-4b29-94dd-8cb692f439dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(yval,  \n",
    "\n",
    "#                                             test_pred_decision_tree)\n",
    "\n",
    "# matrix_df = pd.DataFrame(confusion_matrix)\n",
    "\n",
    "# ax = plt.axes()\n",
    "\n",
    "# sns.set(font_scale=1.3)\n",
    "\n",
    "# plt.figure(figsize=(10,7))\n",
    "\n",
    "# sns.heatmap(matrix_df, annot=True, fmt=\"g\", ax=ax, cmap=\"magma\")\n",
    "\n",
    "# ax.set_title('Confusion Matrix - Decision Tree')\n",
    "\n",
    "# ax.set_xlabel(\"Predicted label\", fontsize =15)\n",
    "\n",
    "# ax.set_xticklabels(labels)\n",
    "\n",
    "# ax.set_ylabel(\"True Label\", fontsize=15)\n",
    "\n",
    "# ax.set_yticklabels(list(labels), rotation = 0)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e101776-b717-4ec8-a1c5-6e47c1fb6bc0",
   "metadata": {},
   "source": [
    "# Simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd24682-16ab-4791-81b2-f064933278f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb2a75-f162-4d4e-aa50-8d307ec0907e",
   "metadata": {},
   "source": [
    "## SimpleCLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f102e19a-af98-4ca8-b198-0b714fe05e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc485a-744b-47f5-9d4d-ac0c1b5cfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn, torch\n",
    "class SimpleCLS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(36, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16327bc6-721f-4179-b996-5caaf985864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fuse_conv(s, out_channel=16):\n",
    "    l = nn.Sequential(\n",
    "        # nn.MaxPool2d(s),\n",
    "        nn.Conv2d(11, out_channel, kernel_size=(s,s),stride=(s,s))\n",
    "        # nn.BatchNorm2d(out_channel),\n",
    "        # nn.ReLU(),\n",
    "    )\n",
    "    return l\n",
    "import torch.nn as nn, torch\n",
    "m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208a331-5561-4fae-8b75-4593559a9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.open(df.iloc[0].img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ae3f6-cecd-4ffc-81aa-5399ff45e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.sample(100)\n",
    "xraw = x = torch.from_numpy(np.array(_df.feat1d.values.tolist())).cpu().float()\n",
    "\n",
    "\n",
    "# x1 = nn.functional.max_pool2d(x1, 4)\n",
    "# x2 = nn.functional.interpolate(x2, (52, 52))\n",
    "# x3 = nn.functional.interpolate(x3, (52, 52))\n",
    "# # x.shape\n",
    "# # fuse = m(x)\n",
    "# # fuse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b1fe6-ec7f-4260-94f0-155072e906ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleCLS2D()(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318929a-a85e-4826-a079-79174f3028f5",
   "metadata": {},
   "source": [
    "[{'id': 1, 'name': 'cigarette'},\n",
    " {'id': 2, 'name': 'food/drink'},\n",
    " {'id': 3, 'name': 'phone'},\n",
    " {'id': 4, 'name': 'face'},\n",
    " {'id': 5, 'name': 'eye'},\n",
    " {'id': 6, 'name': 'mouth'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd9d27-cc22-4152-84a2-393959c19f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x3[0, 5+0].max()\n",
    "\n",
    "# i=6\n",
    "# Image.open(_df.iloc[i].img_path)\n",
    "\n",
    "# lcats = [{'id': 1, 'name': 'cigarette'}, {'id': 2, 'name': 'food/drink'}, {'id': 3, 'name': 'phone'}, {'id': 4, 'name': 'face'}, {'id': 5, 'name': 'eye'}, {'id': 6, 'name': 'mouth'}]\n",
    "\n",
    "# for j in range(6):\n",
    "#     _x = x_fuse[i, j]\n",
    "#     print(lcats[j]['name'], _x.max().item(), _x.shape)\n",
    "#     plt.imshow(_x)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820037a3-b301-4ee8-aeae-aa912e8b361a",
   "metadata": {},
   "source": [
    "## MyLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada74f7-2332-431e-a2d6-d0edaa6616d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLit(LitModel):\n",
    "    pass\n",
    "#     def training_step(self, b, i):\n",
    "        \n",
    "#         x, y = b\n",
    "#         y = y.reshape(-1, 1).float()\n",
    "#         p = self(x)\n",
    "#         with torch.no_grad():\n",
    "#             acc = ((p.sigmoid()>0.5).float()==y.float()).float().mean()\n",
    "#         loss = nn.functional.binary_cross_entropy_with_logits(p.reshape_as(y), y.float())\n",
    "#         self.log('train_acc', acc, prog_bar=True, on_step=True, on_epoch=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self, b, i):\n",
    "#         x, y = b\n",
    "#         y = y.reshape(-1, 1).float()\n",
    "#         p = self(x)\n",
    "#         # loss = nn.functional.binary_cross_entropy_with_logits(p.reshape_as(y), y.float())\n",
    "#         # acc = (p.sigmoid()>0.5).float().mean()\n",
    "#         # acc = ((p.sigmoid()>0.5).float()==y.float()).float().mean()\n",
    "#         self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "#         self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98275d6-2d3f-42fe-bf40-33f76108a9fe",
   "metadata": {},
   "source": [
    "## PLData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d146644-d40d-4608-884b-5d7cbd6ab247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.all import *\n",
    "import torch.utils.data as td\n",
    "import pytorch_lightning as pl\n",
    "from fastcore.all import *\n",
    "\n",
    "def convert_feat18_to_feat15(x):\n",
    "    return x[:,[i for i in range(18) if not i%6==5]].copy()\n",
    "\n",
    "class DS:\n",
    "    def __init__(self, x,y):\n",
    "        # x = convert_feat18_to_feat15(x)\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].astype(np.float32).reshape(-1, 11), self.y[idx].astype(np.int64)\n",
    "\n",
    "class PLData(pl.LightningDataModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        store_attr(**kwargs)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = DS(xtrain, ytrain)\n",
    "        return td.DataLoader(dataset, self.batch_size, num_workers=self.num_workers, drop_last=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = DS(xval, yval)\n",
    "        return td.DataLoader(dataset, self.batch_size, num_workers=self.num_workers,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc66b9e-1e0a-4e83-825d-50721cafcc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60355b2b-977c-4b04-bf7a-71e47ae6c096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4564755-11b3-4474-bb1c-dad042b5e3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yolox.models.yolox.YOLOX"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fef74-0304-4b87-942c-9fa270f839c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0484, 0.4297, 0.2828, 0.2391])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3f675-887e-4e7d-9819-0f5cd215962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.demo import Predictor\n",
    "predictor = Predictor(model, exp)\n",
    "@patch\n",
    "def img_preproc(self:Predictor, img):\n",
    "    img_info = {\"id\": 0}\n",
    "    if isinstance(img, str):\n",
    "        img_info[\"file_name\"] = os.path.basename(img)\n",
    "        img = cv2.imread(img)\n",
    "    else:\n",
    "        img_info[\"file_name\"] = None\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    img_info[\"height\"] = height\n",
    "    img_info[\"width\"] = width\n",
    "    img_info[\"raw_img\"] = img\n",
    "\n",
    "    ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n",
    "    img_info[\"ratio\"] = ratio\n",
    "\n",
    "    img, _ = self.preproc(img, None, self.test_size)\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    img = img.float()\n",
    "    if self.device == \"gpu\":\n",
    "        img = img.cuda()\n",
    "        if self.fp16:\n",
    "            img = img.half()  # to FP16\n",
    "\n",
    "    return img, img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236510e-7924-4a02-a7b3-629cda6e29d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/images/000001.jpg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_info = predictor.img_preproc(df.iloc[0].img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ac415-3522-4458-b3be-9216a3d323a4",
   "metadata": {},
   "source": [
    "## TriStageExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bbfa7-a7af-4b53-840d-50ad8231d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriStageExp(BaseExp):\n",
    "\n",
    "    def __init__(self, exp_name='EXPNAME', \n",
    "                 batch_size=64, \n",
    "                 num_workers=2, \n",
    "                 devices=2,\n",
    "                 strategy='dp', \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        store_attr(**kwargs)\n",
    "\n",
    "    def get_model(self):\n",
    "        dl = self.get_data_loader().train_dataloader()\n",
    "        sched = fn_schedule_cosine_with_warmpup_decay_timm(\n",
    "            num_epochs=self.max_epochs,\n",
    "            num_steps_per_epoch=len(dl)//self.devices,\n",
    "            num_epochs_per_cycle=self.max_epochs//self.num_lr_cycles,\n",
    "            min_lr=1/100,\n",
    "            cycle_decay=0.7,\n",
    "        )\n",
    "        optim = lambda params:torch.optim.Adam(params)\n",
    "\n",
    "        return MyLit(self.model, create_optimizer_fn=optim,\n",
    "                                   create_lr_scheduler_fn=sched, loss_fn=FocalLoss())\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        return PLData(batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def get_trainer(self, **kwargs):\n",
    "        from ple.trainer import get_trainer\n",
    "        return get_trainer(self.exp_name, \n",
    "                              max_epochs=self.max_epochs, \n",
    "                              gpus=self.devices,\n",
    "                           strategy=self.strategy,\n",
    "                           **kwargs,\n",
    "\n",
    "                          )\n",
    "exp = TriStageExp(exp_name='simple_nn', batch_size=256, devices=1, model=SimpleCLS2D(), max_epochs=30)\n",
    "# print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be51077-cf95-42ef-9a17-81cb343c7a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 06:48:55.766 | INFO     | ple.lit_model:fn_schedule_cosine_with_warmpup_decay_timm:66 - num_cycles=3\n",
      "2022-09-20 06:48:55.769 | INFO     | ple.trainer:get_trainer:34 - Log root dir: lightning_logs/simple_nn/40\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lit_model = exp.get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46a55d-77d0-44b3-b986-66ee8ca1bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: lightning_logs/simple_nn/39/tb_logs/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name    | Type        | Params\n",
      "----------------------------------------\n",
      "0 | model   | SimpleCLS2D | 294 K \n",
      "1 | loss_fn | FocalLoss   | 0     \n",
      "----------------------------------------\n",
      "294 K     Trainable params\n",
      "0         Non-trainable params\n",
      "294 K     Total params\n",
      "1.176     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhvth8/.conda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/anhvth8/.conda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b634266346436fa3201caa48f4d3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhvth8/.conda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = exp.get_trainer()\n",
    "trainer.fit(lit_model, exp.get_data_loader());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf29db6-aa8f-4c31-a644-360e0622ac75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.layers.0.weight', 'model.layers.0.bias', 'model.layers.1.weight', 'model.layers.1.bias', 'model.layers.1.running_mean', 'model.layers.1.running_var', 'model.layers.1.num_batches_tracked', 'model.layers.3.weight', 'model.layers.3.bias', 'model.layers.4.weight', 'model.layers.4.bias', 'model.layers.4.running_mean', 'model.layers.4.running_var', 'model.layers.4.num_batches_tracked', 'model.layers.7.weight', 'model.layers.7.bias'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc89400-a261-413f-919e-e700a4cb1d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 06:49:04.058 | INFO     | ple.lit_model:fn_schedule_cosine_with_warmpup_decay_timm:66 - num_cycles=3\n"
     ]
    }
   ],
   "source": [
    "lit_model = exp.get_model();\n",
    "\n",
    "lit_model.load_from_checkpoint('lightning_logs/simple_nn/39/ckpts/epoch=7-val_acc=0.73.ckpt', model=lit_model.model)\n",
    "lit_model = lit_model.cuda().requires_grad_(False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0c929-37a8-4ebe-9650-b4ffdf45bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dms_drowsiness.video_writer import Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5b7bb-2023-4073-8a1e-4fbcd5cd99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "session = onnxruntime.InferenceSession('out.onnx')\n",
    "\n",
    "def predict_onnx(img_path):\n",
    "    img = mmcv.imread(img_path, 0)\n",
    "    img = mmcv.imrescale(img, (416, 416))\n",
    "    img = mmcv.impad(img, shape=(416, 416), pad_val=114)\n",
    "    inp = img[None, :,:,None]\n",
    "    ort_inputs = {session.get_inputs()[0].name: inp.astype(np.float32)}\n",
    "    output = session.run(None, ort_inputs)[0]\n",
    "    pred_cls = np.argmax(output)\n",
    "    score = output[pred_cls]\n",
    "    return score, pred_cls, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa0746-7c29-4958-9b33-d24b895d73c5",
   "metadata": {},
   "source": [
    "### Gửi em ảnh test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c4ff1-e3a0-4bbf-a0eb-4e6b6815b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = mmcv.imread(img_path, 0)\n",
    "# img = mmcv.imrescale(img, (416, 416))\n",
    "# img = mmcv.impad(img, shape=(416, 416), pad_val=114)\n",
    "# mmcv.imwrite(img, '/tmp/guiemanhtest.png')\n",
    "# model_output = predict_onnx(img_path)[-1]\n",
    "# np.save('/tmp/guiemanhtest', model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ab36b-8ed5-4f23-9b39-a3e7ba3e526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_json_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/evamaria_smoking_0032/annotations/pred_mb2_face_food.json',\n",
       " 'label_path': '/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/training/evamaria/smoking/evamaria_smoking_0032.json',\n",
       " 'video_path': '/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/training/evamaria/smoking/evamaria_smoking_0032.mp4',\n",
       " 'raw_feat_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/evamaria_smoking_0032/annotations/pred_mb2_face_food_2_raw_outputs.pkl'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ids = df_train.video_index.unique().tolist()\n",
    "val_json_paths = [list_json_paths[i] for i in val_ids]\n",
    "# val_json_paths\n",
    "json_paths = np.random.choice(val_json_paths)\n",
    "json_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a6482-08db-46a7-8fba-bd936e776da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:04:27.576 | INFO     | avcv.utils:images_to_video:267 - Write video, output_size: (800, 300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1001/1001, 292.4 task/s, elapsed: 3s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:04:31.001 | INFO     | avcv.utils:images_to_video:277 - -> /home/anhvth8/gitprojects/YOLOX/vis.mp4\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "cc = CocoDataset(json_paths['pred_json_path'])\n",
    "label = Label(mmcv.load(json_paths['label_path']), mmcv.VideoReader(json_paths['video_path']))\n",
    "vis_list = []\n",
    "raw_feat = read_raw_feat_one_video(json_paths['raw_feat_path'])\n",
    "def fv(img_id):\n",
    "    board = Board(num_lines=4, line_w=500)\n",
    "    frame = cc.visualize(img_id, score_thr=0.05);\n",
    "    img = cc.gt.imgs[img_id]\n",
    "    img_path = osp.join(cc.img_dir, img['file_name'])\n",
    "    score, pred_cls = predict_onnx(img_path)[:2]\n",
    "    action = _id2action[pred_cls.item()]\n",
    "    if action != 'none':\n",
    "        board.set_line_text(1, action, score.item())\n",
    "    lbl = label.check_action_at_frame_idx(img_id)\n",
    "    if len(lbl):\n",
    "        board.set_line_text(2, f'Label: {lbl[0]}')\n",
    "\n",
    "    vis = board.img_concat(frame)\n",
    "    return vis\n",
    "vis_list = multi_thread(fv, cc.img_ids, 8)\n",
    "clear_output()\n",
    "images_to_video(vis_list, 'vis.mp4', output_size=(800, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59ed09-4ce5-4408-9f1c-f2b05b737348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
