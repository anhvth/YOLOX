{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9dc25-1b38-4db6-8377-e1607ac9c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avcv.all import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eac48b-faf1-4050-bf6c-146ba92eea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label:\n",
    "    def __init__(self, label, video):\n",
    "        self.label = label\n",
    "        self.video = video\n",
    "        \n",
    "    def check_action_at_frame_idx(self, i):\n",
    "        if isinstance(i, int):\n",
    "            i = i/self.video.fps\n",
    "            \n",
    "        actions = []\n",
    "        for action in self.label['annotation']['actionAnnotationList']:\n",
    "            if i >= action['start'] and i < action['end']:\n",
    "                action_idx = action['action']\n",
    "                action_name = self.actionid2name[action_idx]\n",
    "                actions.append(action_name)\n",
    "        return actions\n",
    "    @property\n",
    "    def actionid2name(self):\n",
    "        if hasattr(self, '_actionid2name'):\n",
    "            return self._actionid2name\n",
    "        ret = dict()\n",
    "        for actionLabel in self.label['config']['actionLabelData']:\n",
    "            ret[actionLabel['id']] = actionLabel['name']\n",
    "        self._actionid2name = ret\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b66e82-7a4c-4c6f-b507-b493d4f87155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(label_path, label_type='food'):\n",
    "    if label_type == 'food':\n",
    "        video_name = get_name(label_path)\n",
    "        pred_json_path = f'/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/{video_name}/annotations/pred_mb2_face_food.json'\n",
    "        root_video_name = '_'.join(video_name.split('_')[:-2])\n",
    "        video_path = label_path.replace('.json', '.mp4')\n",
    "        raw_feat_path = pred_json_path.replace('.json', '_2_raw_outputs.pkl')\n",
    "    elif label_type == 'phone/cigarret':\n",
    "        pred_json_path = f'/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/{video_name}/annotations/pred_mb2_face_food.json'\n",
    "        t = 'smoking' if 'smoking' in video_name else 'mobile_usage'\n",
    "        label_path = f'/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/training/yoon/{t}/{video_name}.json'\n",
    "        video_path = label_path.replace('.json', '.mp4')\n",
    "        raw_feat_path = pred_json_path.replace('.json', '_2_raw_outputs.pkl')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    assert osp.exists(pred_json_path), f'404 {pred_json_path}'\n",
    "    assert osp.exists(label_path), f'404 {label_path}'\n",
    "    assert osp.exists(video_path), f'404 {video_path}'\n",
    "    assert osp.exists(raw_feat_path), f'404 {raw_feat_path}'\n",
    "    \n",
    "    return dict(\n",
    "        pred_json_path=pred_json_path,\n",
    "        label_path = label_path,\n",
    "        video_path=video_path,\n",
    "        raw_feat_path=raw_feat_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59871c92-bf9f-4db0-a692-70a37d09d3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 /home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0017/annotations/pred_mb2_face_food_2_raw_outputs.pkl\n",
      "404 /home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hoangnh42_Sensing_Session0_CAMc_1b_2b_3a_4c_5b_6b_7b_8a_9a_10b_11b_12b_13b_14a_15b_16f_17a_18a_19a_20b_eating_0011/annotations/pred_mb2_face_food_2_raw_outputs.pkl\n",
      "len(label_paths)=67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_json_paths = []\n",
    "\n",
    "\n",
    "label_paths  = glob('/data/DMS_Behavior_Detection/RawVideos/Action_Eating/**/*.json', recursive=True)\n",
    "for label_path in label_paths:\n",
    "    try:\n",
    "        json_paths = get_data(label_path, 'food')\n",
    "        list_json_paths.append(json_paths)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "label_paths  = glob('/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/*/**/*.json', recursive=True)\n",
    "print(f'{len(label_paths)=}')\n",
    "for label_path in label_paths:\n",
    "    try:\n",
    "        json_paths = get_data(label_path, 'food')\n",
    "        list_json_paths.append(json_paths)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "len(list_json_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6fdc1-b7d3-4a4d-a43b-a37012352265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_json_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/annotations/pred_mb2_face_food.json',\n",
       " 'label_path': '/data/DMS_Behavior_Detection/RawVideos/Action_Eating/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003.json',\n",
       " 'video_path': '/data/DMS_Behavior_Detection/RawVideos/Action_Eating/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003.mp4',\n",
       " 'raw_feat_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/annotations/pred_mb2_face_food_2_raw_outputs.pkl'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_json_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136e858-8416-45a1-9633-505ad350fc86",
   "metadata": {},
   "source": [
    "## Read 2d video feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eeffe-cb3a-4757-b855-eae0aeb39253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_flatten_to_2d_feature(flatten_sample):\n",
    "    feat_sizes = [416//8, 416//16, 416//32]\n",
    "    # reg_orig_shape = np.array(reg_orig_shape)**2\n",
    "    cur_i = 0\n",
    "    feats = []\n",
    "    for feat_size in feat_sizes:\n",
    "        a = cur_i\n",
    "        b = a+feat_size**2\n",
    "        cur_i = b\n",
    "        feats.append(flatten_sample[a:b].reshape(feat_size, feat_size, -1))\n",
    "    return feats\n",
    "\n",
    "def read_raw_feat_one_video(path):\n",
    "    data = dict(mmcv.load(path))\n",
    "    # for k in data:\n",
    "    #     data[k] = convert_flatten_to_2d_feature(data[k])\n",
    "    return data\n",
    "    # list_json_paths[0]['raw_feat_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299aeee-1815-4238-b023-9400d8dbab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ddfbe-8cea-4686-8dd3-e6504d19d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anns2tensor(img, anns):\n",
    "    h, w = img['height'], img['width']\n",
    "    tensor = []\n",
    "    for ann in anns:\n",
    "        x,y,w,h = ann['bbox']\n",
    "        x /=img['width']\n",
    "        w /=img['width']\n",
    "        y /=img['height']\n",
    "        h /=img['height']\n",
    "        s = ann['score']\n",
    "        cat = ann['category_id']\n",
    "        tensor.append([x,y,w,h,s, cat])\n",
    "    return np.array(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05454863-cca5-436e-8c2e-3b5152f0ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [01:41<00:00,  3.84it/s]2022-09-20 05:01:48.873 | INFO     | avcv.process:multi_thread:33 - multi_thread\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [01:41<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def collect_data(inp):\n",
    "    index, json_paths = inp\n",
    "    v = mmcv.VideoReader(json_paths['video_path'])\n",
    "    # t = len(v)\n",
    "    label = Label(mmcv.load(json_paths['label_path']), v)\n",
    "    cc = CocoDataset(json_paths['pred_json_path'])\n",
    "    data = []\n",
    "    raw_feat2d = read_raw_feat_one_video(json_paths['raw_feat_path'])\n",
    "    for i, frame in enumerate(v):\n",
    "        actions = label.check_action_at_frame_idx(i)\n",
    "        anns = cc.gt.imgToAnns[i]\n",
    "        img = cc.gt.imgs[i]\n",
    "        tensor = None#anns2tensor(img, anns)\n",
    "        img_path = osp.join(cc.img_dir, img['file_name'])\n",
    "        \n",
    "        data.append((tensor, actions[0] if len(actions) else 'none' , \n",
    "                     index, img_path, img['id'], raw_feat2d[img['id']]))\n",
    "        \n",
    "    return data\n",
    "data = multi_thread(collect_data, list(enumerate(list_json_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0160a-4ccd-4919-9d94-7de6c3721474",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for _ in data: all_data += _\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=['tensor', 'action', 'video_index', 'img_path', 'img_id', 'feat1d'])\n",
    "ids = df[df['action'] == 'smocking'].index\n",
    "df.loc[ids, 'action'] = 'smoking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb65c5f-d901-4638-8687-6f77e99a4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = get_exp_by_file('exps/dms/mb2_face_food.py')\n",
    "# model = exp.get_model()\n",
    "# st = torch.load('./YOLOX_outputs/mb2_face_food/best_ckpt.pth')['model']\n",
    "# res = model.load_state_dict(st)\n",
    "# print(res)\n",
    "# model.requires_grad_(False).eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e797d91-ff30-46a1-beb8-d24b1626c860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eating': 0, 'none': 1, 'mobile usage': 2, 'smoking': 3}\n"
     ]
    }
   ],
   "source": [
    "_id2action = dict(enumerate(df['action'].apply(str).unique().tolist()))\n",
    "action2id = {v:k for k, v in _id2action.items()}\n",
    "print(action2id)\n",
    "\n",
    "def get_y(actions):\n",
    "    return action2id[str(actions)]\n",
    "\n",
    "def get_x_tensor(tensor):\n",
    "    zt = np.zeros([1,6], dtype=np.float32)\n",
    "    def get_tensor_cat(cat):\n",
    "        if len(tensor) == 0:\n",
    "            return zt\n",
    "        \n",
    "        _t = tensor[tensor[:,-1] ==cat]\n",
    "        \n",
    "        if len(_t):\n",
    "            max_id = _t[:,-2].argmax()\n",
    "            _t = _t[max_id][None]\n",
    "        else:\n",
    "            return zt\n",
    "            \n",
    "        return _t\n",
    "    return np.concatenate([get_tensor_cat(i) for i in range(1, 7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93e5ba-2878-4c73-95e9-64f079f6d91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b296985-b203-46ef-897d-65dcf49c1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x'] = df.feat1d#multi_process(get_x, df.tensor, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6a2c8-9302-4dce-a91c-429bafafb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = df['action'].apply(get_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a047d-8962-45d5-84ab-caf90b40561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0888634-9639-4a57-bfd2-6753f03ccbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eating', 'none', 'mobile usage', 'smoking']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.action.unique().tolist()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8c818-e077-407d-a4d4-9a5536966ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df_train)=65748, len(df_val)=18277\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df.video_index.apply(lambda i: i % 5 != 0)]\n",
    "df_val = df[df.video_index.apply(lambda i: i % 5 == 0)]\n",
    "print(f'{len(df_train)=}, {len(df_val)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffad62c-9d08-4c43-935e-3af92efc5a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c89bdc-756e-4823-a0d2-81f96dbbe23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d36a1-9ba0-48d7-8116-cff26536a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2xy(df):\n",
    "    return np.array(df.x.values.tolist()).reshape(len(df), -1), np.array(df.y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2387ec8-300d-4ad4-a8c3-3c46ef32fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = df2xy(df_train)\n",
    "xval, yval = df2xy(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882436d5-e6bf-484c-840b-7ef8a2cb096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937d98d-0461-44c8-9c57-d26c7376e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_diabetes\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# # regressor = DecisionTreeRegressor(random_state=0)\n",
    "# # cross_val_score(regressor, X_train, y_train, cv=10)\n",
    "\n",
    "# clf = DecisionTreeClassifier(max_depth=3, random_state = 42)\n",
    "\n",
    "# clf.fit(xtrain, ytrain)\n",
    "\n",
    "# list_new_categories = ['cigarette', 'food/drink', 'phone', 'face', 'eye', 'mouth']\n",
    "\n",
    "# len(df_train), len(df_val)\n",
    "\n",
    "# list_cats = [_['name'] for _ in mmcv.load(list_json_paths[0]['pred_json_path'])['categories']]\n",
    "\n",
    "# from sklearn import tree\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(30,10), facecolor ='k')\n",
    "# feature_names = []\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         part = list_cats\n",
    "#         name = ['x', 'y', 'w', 'h', 'score', 'category'][j]\n",
    "#         feature_names.append('{}-{}'.format(part, name))\n",
    "\n",
    "# a = tree.plot_tree(clf,\n",
    "\n",
    "#                    feature_names = feature_names,\n",
    "\n",
    "#                    class_names = labels,\n",
    "\n",
    "#                    rounded = True,\n",
    "\n",
    "#                    filled = False,\n",
    "\n",
    "#                    fontsize=14\n",
    "#                   )\n",
    "# plt.show()\n",
    "# test_pred_decision_tree = clf.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38094cc1-180a-482a-89e9-93f2bc3b11a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action\n",
       "eating           5358\n",
       "mobile usage    22372\n",
       "none            50551\n",
       "smoking          5744\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('action').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab595b0e-9f78-4b29-94dd-8cb692f439dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(yval,  \n",
    "\n",
    "#                                             test_pred_decision_tree)\n",
    "\n",
    "# matrix_df = pd.DataFrame(confusion_matrix)\n",
    "\n",
    "# ax = plt.axes()\n",
    "\n",
    "# sns.set(font_scale=1.3)\n",
    "\n",
    "# plt.figure(figsize=(10,7))\n",
    "\n",
    "# sns.heatmap(matrix_df, annot=True, fmt=\"g\", ax=ax, cmap=\"magma\")\n",
    "\n",
    "# ax.set_title('Confusion Matrix - Decision Tree')\n",
    "\n",
    "# ax.set_xlabel(\"Predicted label\", fontsize =15)\n",
    "\n",
    "# ax.set_xticklabels(labels)\n",
    "\n",
    "# ax.set_ylabel(\"True Label\", fontsize=15)\n",
    "\n",
    "# ax.set_yticklabels(list(labels), rotation = 0)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e101776-b717-4ec8-a1c5-6e47c1fb6bc0",
   "metadata": {},
   "source": [
    "# Simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd24682-16ab-4791-81b2-f064933278f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb2a75-f162-4d4e-aa50-8d307ec0907e",
   "metadata": {},
   "source": [
    "## SimpleCLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f102e19a-af98-4ca8-b198-0b714fe05e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc485a-744b-47f5-9d4d-ac0c1b5cfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn, torch\n",
    "class SimpleCLS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(36, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16327bc6-721f-4179-b996-5caaf985864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fuse_conv(s, out_channel=16):\n",
    "    l = nn.Sequential(\n",
    "        # nn.MaxPool2d(s),\n",
    "        nn.Conv2d(11, out_channel, kernel_size=(s,s),stride=(s,s))\n",
    "        # nn.BatchNorm2d(out_channel),\n",
    "        # nn.ReLU(),\n",
    "    )\n",
    "    return l\n",
    "import torch.nn as nn, torch\n",
    "m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208a331-5561-4fae-8b75-4593559a9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.open(df.iloc[0].img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ae3f6-cecd-4ffc-81aa-5399ff45e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.sample(100)\n",
    "xraw = x = torch.from_numpy(np.array(_df.feat1d.values.tolist())).cpu().float()\n",
    "\n",
    "\n",
    "# x1 = nn.functional.max_pool2d(x1, 4)\n",
    "# x2 = nn.functional.interpolate(x2, (52, 52))\n",
    "# x3 = nn.functional.interpolate(x3, (52, 52))\n",
    "# # x.shape\n",
    "# # fuse = m(x)\n",
    "# # fuse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b1fe6-ec7f-4260-94f0-155072e906ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleCLS2D()(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318929a-a85e-4826-a079-79174f3028f5",
   "metadata": {},
   "source": [
    "[{'id': 1, 'name': 'cigarette'},\n",
    " {'id': 2, 'name': 'food/drink'},\n",
    " {'id': 3, 'name': 'phone'},\n",
    " {'id': 4, 'name': 'face'},\n",
    " {'id': 5, 'name': 'eye'},\n",
    " {'id': 6, 'name': 'mouth'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd9d27-cc22-4152-84a2-393959c19f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x3[0, 5+0].max()\n",
    "\n",
    "# i=6\n",
    "# Image.open(_df.iloc[i].img_path)\n",
    "\n",
    "# lcats = [{'id': 1, 'name': 'cigarette'}, {'id': 2, 'name': 'food/drink'}, {'id': 3, 'name': 'phone'}, {'id': 4, 'name': 'face'}, {'id': 5, 'name': 'eye'}, {'id': 6, 'name': 'mouth'}]\n",
    "\n",
    "# for j in range(6):\n",
    "#     _x = x_fuse[i, j]\n",
    "#     print(lcats[j]['name'], _x.max().item(), _x.shape)\n",
    "#     plt.imshow(_x)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820037a3-b301-4ee8-aeae-aa912e8b361a",
   "metadata": {},
   "source": [
    "## MyLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada74f7-2332-431e-a2d6-d0edaa6616d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLit(LitModel):\n",
    "    pass\n",
    "#     def training_step(self, b, i):\n",
    "        \n",
    "#         x, y = b\n",
    "#         y = y.reshape(-1, 1).float()\n",
    "#         p = self(x)\n",
    "#         with torch.no_grad():\n",
    "#             acc = ((p.sigmoid()>0.5).float()==y.float()).float().mean()\n",
    "#         loss = nn.functional.binary_cross_entropy_with_logits(p.reshape_as(y), y.float())\n",
    "#         self.log('train_acc', acc, prog_bar=True, on_step=True, on_epoch=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self, b, i):\n",
    "#         x, y = b\n",
    "#         y = y.reshape(-1, 1).float()\n",
    "#         p = self(x)\n",
    "#         # loss = nn.functional.binary_cross_entropy_with_logits(p.reshape_as(y), y.float())\n",
    "#         # acc = (p.sigmoid()>0.5).float().mean()\n",
    "#         # acc = ((p.sigmoid()>0.5).float()==y.float()).float().mean()\n",
    "#         self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "#         self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98275d6-2d3f-42fe-bf40-33f76108a9fe",
   "metadata": {},
   "source": [
    "## PLData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d146644-d40d-4608-884b-5d7cbd6ab247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.all import *\n",
    "import torch.utils.data as td\n",
    "import pytorch_lightning as pl\n",
    "from fastcore.all import *\n",
    "\n",
    "def convert_feat18_to_feat15(x):\n",
    "    return x[:,[i for i in range(18) if not i%6==5]].copy()\n",
    "\n",
    "class DS:\n",
    "    def __init__(self, x,y):\n",
    "        # x = convert_feat18_to_feat15(x)\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].astype(np.float32).reshape(-1, 11), self.y[idx].astype(np.int64)\n",
    "\n",
    "class PLData(pl.LightningDataModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        store_attr(**kwargs)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = DS(xtrain, ytrain)\n",
    "        return td.DataLoader(dataset, self.batch_size, num_workers=self.num_workers, drop_last=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = DS(xval, yval)\n",
    "        return td.DataLoader(dataset, self.batch_size, num_workers=self.num_workers,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc66b9e-1e0a-4e83-825d-50721cafcc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60355b2b-977c-4b04-bf7a-71e47ae6c096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4564755-11b3-4474-bb1c-dad042b5e3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yolox.models.yolox.YOLOX"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fef74-0304-4b87-942c-9fa270f839c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0484, 0.4297, 0.2828, 0.2391])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3f675-887e-4e7d-9819-0f5cd215962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.demo import Predictor\n",
    "predictor = Predictor(model, exp)\n",
    "@patch\n",
    "def img_preproc(self:Predictor, img):\n",
    "    img_info = {\"id\": 0}\n",
    "    if isinstance(img, str):\n",
    "        img_info[\"file_name\"] = os.path.basename(img)\n",
    "        img = cv2.imread(img)\n",
    "    else:\n",
    "        img_info[\"file_name\"] = None\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    img_info[\"height\"] = height\n",
    "    img_info[\"width\"] = width\n",
    "    img_info[\"raw_img\"] = img\n",
    "\n",
    "    ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n",
    "    img_info[\"ratio\"] = ratio\n",
    "\n",
    "    img, _ = self.preproc(img, None, self.test_size)\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    img = img.float()\n",
    "    if self.device == \"gpu\":\n",
    "        img = img.cuda()\n",
    "        if self.fp16:\n",
    "            img = img.half()  # to FP16\n",
    "\n",
    "    return img, img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d52933-743e-418f-b0a7-8a719e8ebb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 416, 416])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236510e-7924-4a02-a7b3-629cda6e29d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/images/000001.jpg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_info = predictor.img_preproc(df.iloc[0].img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f4386-49c7-494a-ad18-39cad4bb3535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f40c0-89eb-4659-b14c-6aa78141c775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3549, 11])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "096ac415-3522-4458-b3be-9216a3d323a4",
   "metadata": {},
   "source": [
    "## TriStageExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bbfa7-a7af-4b53-840d-50ad8231d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriStageExp(BaseExp):\n",
    "\n",
    "    def __init__(self, exp_name='EXPNAME', \n",
    "                 batch_size=64, \n",
    "                 num_workers=2, \n",
    "                 devices=2,\n",
    "                 strategy='dp', \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        store_attr(**kwargs)\n",
    "\n",
    "    def get_model(self):\n",
    "        dl = self.get_data_loader().train_dataloader()\n",
    "        sched = fn_schedule_cosine_with_warmpup_decay_timm(\n",
    "            num_epochs=self.max_epochs,\n",
    "            num_steps_per_epoch=len(dl)//self.devices,\n",
    "            num_epochs_per_cycle=self.max_epochs//self.num_lr_cycles,\n",
    "            min_lr=1/100,\n",
    "            cycle_decay=0.7,\n",
    "        )\n",
    "        optim = lambda params:torch.optim.Adam(params)\n",
    "\n",
    "        return MyLit(self.model, create_optimizer_fn=optim,\n",
    "                                   create_lr_scheduler_fn=sched, loss_fn=FocalLoss())\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        return PLData(batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def get_trainer(self, **kwargs):\n",
    "        from ple.trainer import get_trainer\n",
    "        return get_trainer(self.exp_name, \n",
    "                              max_epochs=self.max_epochs, \n",
    "                              gpus=self.devices,\n",
    "                           strategy=self.strategy,\n",
    "                           **kwargs,\n",
    "\n",
    "                          )\n",
    "exp = TriStageExp(exp_name='simple_nn', batch_size=256, devices=1, model=SimpleCLS2D(), max_epochs=30)\n",
    "# print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be51077-cf95-42ef-9a17-81cb343c7a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 06:48:55.766 | INFO     | ple.lit_model:fn_schedule_cosine_with_warmpup_decay_timm:66 - num_cycles=3\n",
      "2022-09-20 06:48:55.769 | INFO     | ple.trainer:get_trainer:34 - Log root dir: lightning_logs/simple_nn/40\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lit_model = exp.get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46a55d-77d0-44b3-b986-66ee8ca1bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: lightning_logs/simple_nn/39/tb_logs/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name    | Type        | Params\n",
      "----------------------------------------\n",
      "0 | model   | SimpleCLS2D | 294 K \n",
      "1 | loss_fn | FocalLoss   | 0     \n",
      "----------------------------------------\n",
      "294 K     Trainable params\n",
      "0         Non-trainable params\n",
      "294 K     Total params\n",
      "1.176     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhvth8/.conda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/anhvth8/.conda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b634266346436fa3201caa48f4d3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhvth8/.conda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = exp.get_trainer()\n",
    "trainer.fit(lit_model, exp.get_data_loader());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf29db6-aa8f-4c31-a644-360e0622ac75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.layers.0.weight', 'model.layers.0.bias', 'model.layers.1.weight', 'model.layers.1.bias', 'model.layers.1.running_mean', 'model.layers.1.running_var', 'model.layers.1.num_batches_tracked', 'model.layers.3.weight', 'model.layers.3.bias', 'model.layers.4.weight', 'model.layers.4.bias', 'model.layers.4.running_mean', 'model.layers.4.running_var', 'model.layers.4.num_batches_tracked', 'model.layers.7.weight', 'model.layers.7.bias'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc89400-a261-413f-919e-e700a4cb1d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 06:49:04.058 | INFO     | ple.lit_model:fn_schedule_cosine_with_warmpup_decay_timm:66 - num_cycles=3\n"
     ]
    }
   ],
   "source": [
    "lit_model = exp.get_model();\n",
    "\n",
    "lit_model.load_from_checkpoint('lightning_logs/simple_nn/39/ckpts/epoch=7-val_acc=0.73.ckpt', model=lit_model.model)\n",
    "lit_model = lit_model.cuda().requires_grad_(False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0c929-37a8-4ebe-9650-b4ffdf45bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dms_drowsiness.video_writer import Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d30475-68fa-4f60-839a-75054eb4fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lit_model(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3e88c-cb41-4533-b55d-96eaf4cbe288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_json_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hamid_smoking_0158/annotations/pred_mb2_face_food.json',\n",
       " 'label_path': '/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/training/hamid/smoking/hamid_smoking_0158.json',\n",
       " 'video_path': '/data/DMS_Behavior_Detection/mobile_cigarret_foreignerUS/training/hamid/smoking/hamid_smoking_0158.mp4',\n",
       " 'raw_feat_path': '/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hamid_smoking_0158/annotations/pred_mb2_face_food_2_raw_outputs.pkl'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ids = df_train.video_index.unique().tolist()\n",
    "val_json_paths = [list_json_paths[i] for i in val_ids]\n",
    "# json_paths = list_json_paths[val_ids[-4]]\n",
    "# json_paths = [j for j in val_json_paths if '' in get_name(j['label_path'])][0]\n",
    "# val_json_paths\n",
    "json_paths = np.random.choice(val_json_paths)\n",
    "json_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da8c4e-3bc5-4d41-a195-a88515ec05d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ef5c8-ae61-4df7-9c5a-0a58821e3761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 10 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 6 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "head.bcewithlog_loss, head.iou_loss, head.l1_loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAT CODE- FLOP: 0.2902592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 07:06:14.019 | INFO     | __main__:get_model:20 - loading checkpoint done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([1, 416, 416, 3])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3f0a1-707f-47a4-9b38-e6da76bc786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = get_exp_by_file('exps/dms/mb2_face_food.py')\n",
    "# model = exp.get_model()\n",
    "# st = torch.load('YOLOX_outputs/mb2_face_food/best_ckpt.pth')['model']\n",
    "# model.load_state_dict(st)\n",
    "# model.eval().requires_grad_(False);\n",
    "# mb2_yolox = model.cpu()\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mb2_yolox = mb2_yolox\n",
    "        self.cls = lit_model.model.eval().cpu()\n",
    "    def forward(self, img):\n",
    "        x = self.mb2_yolox(img)\n",
    "        x = self.cls(x)\n",
    "        return x\n",
    "    \n",
    "model_wraper = ModelWrapper().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651c431-23e6-475c-b571-cf685566de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model_wraper,\n",
    "                    torch.randn(1, 1, 416, 416),\n",
    "                    'yolox_mb2_classifer_4_softmax',\n",
    "                    export_params=True,\n",
    "                    opset_version=11,\n",
    "                    # do_constant_folding=True,\n",
    "                    input_names = ['input'],\n",
    "                    output_names = ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073dcee1-15d2-47ee-80b2-e8683a778d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/images/000001.jpg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d612a2e-fc28-4ca0-bf2d-fe3e3055a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = predictor.img_preproc('/home/anhvth8/gitprojects/YOLOX/.cache/raw_video_predict_face_food/hungng_Sensing_Session0_CAMc_1b_2c_3a_4c_5b_6b_7b_8a_9a_10a_11b_12a_13a_14b_15b_16f_17a_18a_19a_20b_eating_0003/images/000001.jpg')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721ecf8-c497-494b-a2f2-6b70bef726e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wraper.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38de25-249c-44a3-ac88-c48520057534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(0.9986, grad_fn=<MaxBackward0>),\n",
       "indices=tensor(0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wraper(input[:,:1]).squeeze().softmax(0).max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5a38d-0d02-4d7c-9ce6-e8490397f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cc.gt.imgs[0] \n",
    "# img_path = osp.join(cc.img_dir, img['file_name'])\n",
    "# inp = predictor.img_preproc(img_path)[0].cuda()\n",
    "# model_wraper(inp)\n",
    "# img_path = osp.join(cc.img_dir, img['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a6482-08db-46a7-8fba-bd936e776da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(0.9235, device='cuda:0'),\n",
       "indices=tensor(3, device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wraper(inp).max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceea21c-46f5-46d3-ae33-e1822192cbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                                                    | 0/917 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [433]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     vis \u001b[38;5;241m=\u001b[39m board\u001b[38;5;241m.\u001b[39mimg_concat(frame)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vis\n\u001b[0;32m---> 29\u001b[0m vis_list \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m clear_output()\n\u001b[1;32m     31\u001b[0m images_to_video(vis_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvis.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m300\u001b[39m))\n",
      "File \u001b[0;32m~/avcv/avcv/process.py:28\u001b[0m, in \u001b[0;36mmulti_thread\u001b[0;34m(fn, array_inputs, max_workers, desc, unit, verbose, pbar_iterval)\u001b[0m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(_wraper, array_inputs)):\n\u001b[1;32m     29\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mupdate(result)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m verbose:\u001b[38;5;66;03m# and i%pbar_iterval==0:\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py38/lib/python3.8/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/.conda/envs/py38/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/.conda/envs/py38/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py38/lib/python3.8/concurrent/futures/thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/avcv/avcv/process.py:21\u001b[0m, in \u001b[0;36mmulti_thread.<locals>._wraper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wraper\u001b[39m(x):\n\u001b[1;32m     20\u001b[0m     i, \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {i: \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m}\n",
      "Input \u001b[0;32mIn [433]\u001b[0m, in \u001b[0;36mfv\u001b[0;34m(img_id)\u001b[0m\n\u001b[1;32m     18\u001b[0m inp \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mimg_preproc(img_path)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     19\u001b[0m score, pred_cls \u001b[38;5;241m=\u001b[39m model_wraper(inp)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m action \u001b[38;5;241m=\u001b[39m _id2action[\u001b[43mpred_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     22\u001b[0m     board\u001b[38;5;241m.\u001b[39mset_line_text(\u001b[38;5;241m1\u001b[39m, action, score\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "cc = CocoDataset(json_paths['pred_json_path'])\n",
    "label = Label(mmcv.load(json_paths['label_path']), mmcv.VideoReader(json_paths['video_path']))\n",
    "# lit_model.cpu()\n",
    "model_wraper.cuda()\n",
    "vis_list = []\n",
    "raw_feat = read_raw_feat_one_video(json_paths['raw_feat_path'])\n",
    "def fv(img_id):\n",
    "    board = Board(num_lines=4, line_w=500)\n",
    "    frame = cc.visualize(img_id, score_thr=0.05);\n",
    "    # anns = cc.gt.imgToAnns[img_id]\n",
    "    img = cc.gt.imgs[img_id]\n",
    "    # feat = get_x(anns2tensor(img, anns)).flatten()[None]\n",
    "    # feat = raw_feat[img_id]\n",
    "    # feat = torch.from_numpy(feat).cpu().float().reshape([1,-1, 11])\n",
    "    # score, pred_cls = lit_model(feat).softmax(1).max(1)\n",
    "    img_path = osp.join(cc.img_dir, img['file_name'])\n",
    "    inp = predictor.img_preproc(img_path)[0].cuda()\n",
    "    score, pred_cls = model_wraper(inp).max(0)\n",
    "    action = _id2action[pred_cls.item()]\n",
    "    if action != 'none':\n",
    "        board.set_line_text(1, action, score.item())\n",
    "    lbl = label.check_action_at_frame_idx(img_id)\n",
    "    if len(lbl):\n",
    "        board.set_line_text(2, f'Label: {lbl[0]}')\n",
    "\n",
    "    vis = board.img_concat(frame)\n",
    "    return vis\n",
    "vis_list = multi_thread(fv, cc.img_ids, 1)\n",
    "clear_output()\n",
    "images_to_video(vis_list, 'vis.mp4', output_size=(800, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57490969-c08c-4c45-9948-efedfc9b45ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0157da9-6a5d-403e-9783-9e426336eed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0b8af-0606-49bc-bfe9-45a48696b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919e4da-c3be-46b6-8fe5-0243b4415cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59ed09-4ce5-4408-9f1c-f2b05b737348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
